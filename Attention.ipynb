{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMmuz8xBfx/9qFlculETBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs20m016/cs6910-Assignment-3/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woghtyvvhqHW"
      },
      "source": [
        "!pip install wandb -qq\n",
        "import wandb"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2-Qg9PAhrtF",
        "outputId": "e0512f75-b856-4191-aa2a-2052e20a0197"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pyoy6ZRWYdK",
        "outputId": "5ad098bd-42e4-4b23-c9e3-83f51a379228"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0YVFzOVWgSU",
        "outputId": "806e4ac0-4e2d-4c5c-fcef-39fa4df698de"
      },
      "source": [
        "!unzip /content/drive/MyDrive/lexicons.zip > /dev/null"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace lexicons/hi.translit.sampled.dev.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWAEy9PWWiB3"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, RNN, GRU, SimpleRNN\n",
        "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.python.keras.models import Model\n",
        "import math\n",
        "from math import log"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZyVcmmeWjoG",
        "outputId": "ea1b07ab-fea6-4676-b9d2-369516407984"
      },
      "source": [
        "# Path to the data txt file on disk.\n",
        "data_path = \"/content/lexicons/hi.translit.sampled.train.tsv\"\n",
        "val_path = \"/content/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_path = \"/content/lexicons/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    val_input_texts.append(input_text)\n",
        "    val_target_texts.append(target_text)\n",
        "\n",
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    test_input_texts.append(input_text)\n",
        "    test_target_texts.append(target_text)\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of validation samples:\", len(val_input_texts))\n",
        "print(\"Number of test samples:\", len(test_input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 44204\n",
            "Number of validation samples: 4358\n",
            "Number of test samples: 4358\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itsnRS5QWlM4",
        "outputId": "95c4ca68-f04f-4f06-cbdf-603f6396f9ad"
      },
      "source": [
        "print(\"Train Set Samples\")\n",
        "for i in range (5):\n",
        "  index = random. randint(0,len(target_texts))\n",
        "  print(input_texts[index],target_texts[index])"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Set Samples\n",
            "mustafa \tमुस्तफा\n",
            "\n",
            "germano \tजर्मनों\n",
            "\n",
            "cdpo \tसीडीपीओ\n",
            "\n",
            "tampo \tटेंपो\n",
            "\n",
            "bhuna \tभूना\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHrKjZwrWmqW",
        "outputId": "f361e3f9-2319-4441-cfc3-0ec8f0528ce6"
      },
      "source": [
        "print(\"Validation Set Samples\")\n",
        "for i in range (5):\n",
        "  index = random. randint(0,len(val_target_texts))\n",
        "  print(val_input_texts[index],val_target_texts[index])"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Set Samples\n",
            "antenna \tएंटेना\n",
            "\n",
            "batala \tबाटला\n",
            "\n",
            "lauch \tलॉच\n",
            "\n",
            "cardinalon \tकार्डिनलों\n",
            "\n",
            "marte \tमारते\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teZx8v6EWoCS",
        "outputId": "776da5e1-54f5-480d-8b7a-aacdbc4cfd87"
      },
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "print(np.shape(encoder_input_data),np.shape(decoder_input_data),np.shape(decoder_target_data))\n",
        "\n",
        "val_encoder_input_data = np.zeros((len(val_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "val_decoder_input_data = np.zeros((len(val_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "val_decoder_target_data = np.zeros((len(val_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "print(np.shape(val_encoder_input_data),np.shape(val_decoder_input_data),np.shape(val_decoder_target_data))\n",
        "\n",
        "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "print(np.shape(test_encoder_input_data))\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "for i, (val_input_text, val_target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(val_input_text):\n",
        "        val_encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    val_encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(val_target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "for i, (test_input_text, ) in enumerate(zip(test_input_texts, )):\n",
        "    for t, char in enumerate(test_input_text):\n",
        "        test_encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    test_encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44204, 20, 27) (44204, 21, 66) (44204, 21, 66)\n",
            "(4358, 20, 27) (4358, 21, 66) (4358, 21, 66)\n",
            "(4358, 20, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRtPl5CLWsuy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbu6FC1PagVc"
      },
      "source": [
        "def GRU_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout):\n",
        "  # Define an input sequence and process it by going through a len(latent_dims)-layer deep encoder\n",
        "  encoder_inputs = Input(shape=(None, num_encoder_tokens),name='encoder_inputs')\n",
        "  decoder_inputs = Input(shape=(None,num_decoder_tokens),name='decoder_inputs')\n",
        "  \n",
        "  encoder_gru = GRU(latent_dims, return_sequences=True, return_state=True,name='encoder_gru',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
        "  decoder_gru = GRU(latent_dims, return_sequences=True, return_state=True,name='decoder_gru',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
        "\n",
        "  # Attention layer\n",
        "  attn_layer = AttentionLayer(name='attention_layer')\n",
        "  attn_out,_ = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "  decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_out, attn_out])\n",
        "\n",
        "  # Dense layer\n",
        "  dense = Dense(num_decoder_tokens, activation='softmax',name='softmax_layer')\n",
        "  dense_time = TimeDistributed(dense)\n",
        "  decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "  # Full model\n",
        "  model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "  model.compile(optimizer=optimizer_fn, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data))\n",
        "  \"\"\" Encoder (Inference) model \"\"\"\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_inf_out, encoder_inf_state = model.layers[2].output\n",
        "  encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "  \"\"\" Decoder (Inference) model \"\"\"\n",
        "  decoder_inf_inputs = model.input[1]\n",
        "  encoder_inf_states = Input(shape=(None, latent_dims))\n",
        "  decoder_init_state = Input(shape=(latent_dims,))\n",
        "  decoder_gru=model.layers[3]\n",
        "  decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
        "  attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "  decoder_inf_concat = Concatenate(axis=-1,name='concat')([decoder_inf_out, attn_inf_out])\n",
        "  decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "  decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "  return model,encoder_model,decoder_model"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6gtKt80Wvkm"
      },
      "source": [
        "def LSTM_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout):\n",
        "  encoder_inputs = Input(shape=(None, num_encoder_tokens),name='encoder_inputs')\n",
        "  decoder_inputs = Input(shape=(None,num_decoder_tokens),name='decoder_inputs')\n",
        " \n",
        "  encoder_lstm = LSTM(latent_dims, return_sequences=True, return_state=True,name='encoder_lstm',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  encoder_out, encoder_stateh , encoder_statec = encoder_lstm(encoder_inputs)\n",
        "  encoder_state = [encoder_stateh , encoder_statec]\n",
        "  decoder_lstm = LSTM(latent_dims, return_sequences=True, return_state=True,name='decoder_lstm',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  decoder_out, decoder_stateh, decoder_statec = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n",
        "\n",
        "  # Attention layer\n",
        "  attn_layer = AttentionLayer(name='attention_layer')\n",
        "  attn_out,_ = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "  decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_out, attn_out])\n",
        "\n",
        "  # Dense layer\n",
        "  dense = Dense(num_decoder_tokens, activation='softmax',name='softmax_layer')\n",
        "  dense_time = TimeDistributed(dense)\n",
        "  decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "  # Full model\n",
        "  model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "  model.compile(optimizer=optimizer_fn, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data))\n",
        "  \"\"\" Encoder (Inference) model \"\"\"\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_inf_out, encoder_inf_stateh, encoder_inf_statec = model.layers[2].output\n",
        "  encoder_inf_state = [ encoder_inf_stateh, encoder_inf_statec]\n",
        "  encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "  \"\"\" Decoder (Inference) model \"\"\"\n",
        "  decoder_inf_inputs = model.input[1]\n",
        "  encoder_inf_states = Input(shape=(None, latent_dims))\n",
        "  decoder_init_state_h = Input(shape=(latent_dims,))\n",
        "  decoder_init_state_c = Input(shape=(latent_dims,))\n",
        "  decoder_init_state = [decoder_init_state_h,decoder_init_state_c]\n",
        "  decoder_lstm=model.layers[3]\n",
        "  decoder_inf_out, decoder_inf_stateh, decoder_inf_statec = decoder_lstm(decoder_inf_inputs, initial_state=decoder_init_state)\n",
        "  decoder_inf_state = [decoder_inf_stateh, decoder_inf_statec]\n",
        "  attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "  decoder_inf_concat = Concatenate(axis=-1,name='concat')([decoder_inf_out, attn_inf_out])\n",
        "  decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "  decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "  return model,encoder_model,decoder_model"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FniVAMKTbSOq"
      },
      "source": [
        "def RNN_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout):\n",
        "  # Define an input sequence and process it by going through a len(latent_dims)-layer deep encoder\n",
        "  encoder_inputs = Input(shape=(None, num_encoder_tokens),name='encoder_inputs')\n",
        "  decoder_inputs = Input(shape=(None,num_decoder_tokens),name='decoder_inputs')\n",
        "  \n",
        "  encoder_gru = SimpleRNN(latent_dims, return_sequences=True, return_state=True,name='encoder_rnn',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
        "  decoder_gru = SimpleRNN(latent_dims, return_sequences=True, return_state=True,name='decoder_rnn',dropout=dropout,recurrent_dropout=recc_dropout)\n",
        "  decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
        "\n",
        "  # Attention layer\n",
        "  attn_layer = AttentionLayer(name='attention_layer')\n",
        "  attn_out,_ = attn_layer([encoder_out, decoder_out])\n",
        "\n",
        "  decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_out, attn_out])\n",
        "\n",
        "  # Dense layer\n",
        "  dense = Dense(num_decoder_tokens, activation='softmax',name='softmax_layer')\n",
        "  dense_time = TimeDistributed(dense)\n",
        "  decoder_pred = dense_time(decoder_concat_input)\n",
        "\n",
        "  # Full model\n",
        "  model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
        "  model.compile(optimizer=optimizer_fn, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_data=([val_encoder_input_data, val_decoder_input_data],val_decoder_target_data))\n",
        "  \"\"\" Encoder (Inference) model \"\"\"\n",
        "  encoder_inputs = model.input[0]\n",
        "  encoder_inf_out, encoder_inf_state = model.layers[2].output\n",
        "  encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
        "\n",
        "  \"\"\" Decoder (Inference) model \"\"\"\n",
        "  decoder_inf_inputs = model.input[1]\n",
        "  encoder_inf_states = Input(shape=(None, latent_dims))\n",
        "  decoder_init_state = Input(shape=(latent_dims,))\n",
        "  decoder_gru=model.layers[3]\n",
        "  decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
        "  attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
        "  decoder_inf_concat = Concatenate(axis=-1,name='concat')([decoder_inf_out, attn_inf_out])\n",
        "  decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
        "  decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
        "  return model,encoder_model,decoder_model"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMHYNJslW4PN"
      },
      "source": [
        "def decode_sequence_attention(input_seq,beam_search,encoder_model,decoder_model,reverse_target_char_index):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_outs, enc_last_state = encoder_model.predict(input_seq)\n",
        "    dec_state = enc_last_state\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentences = []\n",
        "    attention_weights = []\n",
        "    if beam_search==0:\n",
        "      decoded_sentence = []  #Creating a list then using \"\".join() is usually much faster for string creation\n",
        "      while not stop_condition:\n",
        "          dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, target_seq])\n",
        "          dec_ind = np.argmax(dec_out[0, 0])\n",
        "          attention_weights.append((dec_ind, attention))\n",
        "          sampled_char = reverse_target_char_index[dec_ind]\n",
        "          decoded_sentence.append(sampled_char)\n",
        "          if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "          target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "          target_seq[0, 0, dec_ind] = 1.\n",
        "      decoded_sentences.append(\"\".join(decoded_sentence))\n",
        "    else:\n",
        "      decoded_prob = []\n",
        "      it=0\n",
        "      while not stop_condition:\n",
        "          dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, target_seq])\n",
        "          dec_ind = np.argmax(dec_out[0, 0])\n",
        "          attention_weights.append((dec_ind, attention))\n",
        "          sampled_char = reverse_target_char_index[dec_ind]\n",
        "          it += 1\n",
        "          decoded_prob.append(dec_out[0, 0])\n",
        "          if sampled_char == '\\n' or it > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "          target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "          target_seq[0, 0, dec_ind] = 1.\n",
        "      sequences = [[list(), 0.0]]\n",
        "      # walk over each step in sequence\n",
        "      for row in decoded_prob:\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "          for j in range(len(row)):\n",
        "            candidate = [seq + [j], score - log(row[j])]\n",
        "            all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        # select k best\n",
        "        sequences = ordered[:beam_search]\n",
        "      for seq in sequences:\n",
        "        decoded_sentence = []\n",
        "        for char_in in seq[0]:\n",
        "          sampled_char = reverse_target_char_index[char_in]\n",
        "          decoded_sentence.append(sampled_char)\n",
        "        decoded_sentences.append(\"\".join(decoded_sentence))\n",
        "    return decoded_sentences"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRALVkwsd82l"
      },
      "source": [
        "def RNNatt(epochs, batch_size, dropout, recc_dropout, beam_search, latent_dims, cell_type, optimizer_fn):\n",
        "  if cell_type=='LSTM':\n",
        "    model,encoder_model,decoder_model = LSTM_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout)\n",
        "  elif cell_type=='GRU':\n",
        "    model,encoder_model,decoder_model = GRU_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout)\n",
        "  elif cell_type=='RNN':\n",
        "    model,encoder_model,decoder_model = RNN_attention(epochs, batch_size, latent_dims, optimizer_fn, dropout, recc_dropout)\n",
        "  # Reverse-lookup token index to decode sequences back to\n",
        "  # something readable.\n",
        "  reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "  reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "  valid = 0\n",
        "  samples=1000\n",
        "  for i in range(samples):\n",
        "      # Take one sequence (part of the training set)\n",
        "      # for trying out decoding.\n",
        "      seq_index=random.randint(0,len(test_input_texts))\n",
        "      #seq_index=i\n",
        "      input_seq = test_encoder_input_data[seq_index : seq_index + 1]\n",
        "      decoded_sentence = decode_sequence_attention(input_seq,beam_search,encoder_model,decoder_model,reverse_target_char_index)\n",
        "      #print(\"-\")\n",
        "      #print(\"Input sentence:\", test_input_texts[seq_index])\n",
        "      #for seq in decoded_sentence:\n",
        "        #print(\"Decoded sentence:\", seq)\n",
        "      #print(\"Actual sentence:\", test_target_texts[seq_index])\n",
        "      for translate in decoded_sentence:\n",
        "        flag=1\n",
        "        for (i,j) in zip(translate,test_target_texts[seq_index]):\n",
        "          if i!=j:\n",
        "            flag=0\n",
        "            break\n",
        "        if flag==1:\n",
        "          valid += 1\n",
        "          break\n",
        "  print(\"Word Level Validation Accuracy :\",(valid/samples)*100)\n",
        "  return (valid/samples)*100"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wda83qLhVw7"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'model_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epoch': {\n",
        "            'values': [8,10]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64]\n",
        "        },\n",
        "        'dropout':{\n",
        "            'values': [0.1,0.01,0.0]\n",
        "        },\n",
        "        'recc_dropout':{\n",
        "            'values': [0.1,0.2,0.05]\n",
        "        },\n",
        "        'beam_search': {\n",
        "            'values': [3,4,5,6]\n",
        "        },\n",
        "        'latent_dims':{\n",
        "            'values': [64,126,256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values':['GRU','LSTM','RNN']\n",
        "        },\n",
        "        'optimizer_fn': {\n",
        "            'values': ['adam','rmsprop']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Navli8g6hfQJ",
        "outputId": "7f581987-5f98-406b-dee3-edf04536cc6e"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config,entity=\"dl-assignment3\",project=\"Attention\")"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: c0grp5r2\n",
            "Sweep URL: https://wandb.ai/dl-assignment3/Attention/sweeps/c0grp5r2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMWF_ztshiRk"
      },
      "source": [
        "def train():\n",
        "    config_defaults={\n",
        "        'epoch':10,\n",
        "        'batch_size':64,\n",
        "        'dropout':0.01,\n",
        "        'recc_dropout':0.1,\n",
        "        'beam_search':3,\n",
        "        'latent_dims':64,\n",
        "        'cell_type':'LSTM',\n",
        "        'optimizer_fn':'adam',\n",
        "        }\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    model_acc=RNNatt(config.epoch, config.batch_size,config.dropout, config.recc_dropout, config.beam_search,config.latent_dims, config.cell_type, config.optimizer_fn)  \n",
        "    wandb.log({'model_accuracy': model_acc})\n",
        "    return"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "T7b2YRLUhnC4",
        "outputId": "8fe0e8ac-8b95-4d7a-e9e9-d7ce5efbb6b5"
      },
      "source": [
        "wandb.agent(sweep_id, train,count=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 15rmklnt with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dims: 126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_fn: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecc_dropout: 0.1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">serene-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dl-assignment3/Attention\" target=\"_blank\">https://wandb.ai/dl-assignment3/Attention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/dl-assignment3/Attention/sweeps/c0grp5r2\" target=\"_blank\">https://wandb.ai/dl-assignment3/Attention/sweeps/c0grp5r2</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/dl-assignment3/Attention/runs/15rmklnt\" target=\"_blank\">https://wandb.ai/dl-assignment3/Attention/runs/15rmklnt</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210521_073259-15rmklnt</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 88/691 [==>...........................] - ETA: 2:18 - loss: 2.0190 - accuracy: 0.6230"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}